{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "3796a599-0e27-4a30-829d-4bff96857725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort (from gpt-2-simple)\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.32.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24559 sha256=10c08497dd880e396061bc416103b804b00361819cccd93b51cb4ec3a91678a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/6a/fe/10d3223f78d1ac3e4c83bb4c5e2d28dfb1789c2fb4cc7ea8d0\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "151347bc-363b-47c7-9cd4-219876c65d77"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 832Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 2.47Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 765Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:13, 36.1Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 446Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 3.71Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 3.70Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "b5741e1d-6772-4786-e11d-0c7164d4e960"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "20ba1d5b-4f9d-4669-acc3-efd1bd446195"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "I think the answer is that there are no planets, so there's no need to do any further searches. So, yes, there are a couple of planets in space. And one of those is very close to the Sun. And one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "498d72d4-7126-47de-f361-4f75e6b6f90c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-24 11:33:03--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.217.24, 52.216.76.30, 52.216.207.53, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.217.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txtâ€™\n",
            "\n",
            "nietzsche.txt       100%[===================>] 586.82K  1.85MB/s    in 0.3s    \n",
            "\n",
            "2023-05-24 11:33:03 (1.85 MB/s) - â€˜nietzsche.txtâ€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "2d9f32f6-770a-49e5-a0ca-edaf556d124c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 201MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "cf121d57-95ad-4452-8b9f-785c0dee27b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-24 11:33:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-05-24 11:33:13 (37.6 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'got1.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "e726d500-0cc0-4141-dffd-0522a9e5ea2c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 433157 tokens\n",
            "Training...\n",
            "[1 | 6.39] loss=3.52 avg=3.52\n",
            "[2 | 8.50] loss=3.28 avg=3.40\n",
            "[3 | 10.60] loss=3.43 avg=3.41\n",
            "[4 | 12.71] loss=3.46 avg=3.42\n",
            "[5 | 14.82] loss=3.32 avg=3.40\n",
            "[6 | 16.94] loss=3.32 avg=3.39\n",
            "[7 | 19.07] loss=3.21 avg=3.36\n",
            "[8 | 21.22] loss=3.09 avg=3.33\n",
            "[9 | 23.36] loss=3.15 avg=3.31\n",
            "[10 | 25.50] loss=3.06 avg=3.28\n",
            "[11 | 27.65] loss=3.10 avg=3.26\n",
            "[12 | 29.81] loss=3.24 avg=3.26\n",
            "[13 | 31.98] loss=3.15 avg=3.25\n",
            "[14 | 34.14] loss=3.22 avg=3.25\n",
            "[15 | 36.31] loss=3.07 avg=3.24\n",
            "[16 | 38.48] loss=3.15 avg=3.23\n",
            "[17 | 40.67] loss=3.22 avg=3.23\n",
            "[18 | 42.86] loss=3.10 avg=3.22\n",
            "[19 | 45.07] loss=3.13 avg=3.22\n",
            "[20 | 47.26] loss=3.14 avg=3.21\n",
            "[21 | 49.47] loss=3.11 avg=3.21\n",
            "[22 | 51.68] loss=3.22 avg=3.21\n",
            "[23 | 53.89] loss=3.00 avg=3.20\n",
            "[24 | 56.12] loss=3.07 avg=3.19\n",
            "[25 | 58.36] loss=3.00 avg=3.18\n",
            "[26 | 60.60] loss=3.07 avg=3.18\n",
            "[27 | 62.84] loss=3.00 avg=3.17\n",
            "[28 | 65.09] loss=3.00 avg=3.16\n",
            "[29 | 67.34] loss=3.03 avg=3.16\n",
            "[30 | 69.61] loss=3.06 avg=3.16\n",
            "[31 | 71.88] loss=3.06 avg=3.15\n",
            "[32 | 74.15] loss=3.01 avg=3.15\n",
            "[33 | 76.44] loss=3.08 avg=3.14\n",
            "[34 | 78.73] loss=3.15 avg=3.14\n",
            "[35 | 81.04] loss=3.00 avg=3.14\n",
            "[36 | 83.35] loss=3.03 avg=3.14\n",
            "[37 | 85.67] loss=3.08 avg=3.13\n",
            "[38 | 88.00] loss=2.90 avg=3.13\n",
            "[39 | 90.35] loss=2.89 avg=3.12\n",
            "[40 | 92.71] loss=3.12 avg=3.12\n",
            "[41 | 95.06] loss=3.02 avg=3.12\n",
            "[42 | 97.43] loss=3.18 avg=3.12\n",
            "[43 | 99.81] loss=3.11 avg=3.12\n",
            "[44 | 102.19] loss=2.99 avg=3.11\n",
            "[45 | 104.58] loss=2.94 avg=3.11\n",
            "[46 | 106.98] loss=2.90 avg=3.10\n",
            "[47 | 109.36] loss=3.04 avg=3.10\n",
            "[48 | 111.74] loss=3.10 avg=3.10\n",
            "[49 | 114.10] loss=3.02 avg=3.10\n",
            "[50 | 116.45] loss=2.90 avg=3.09\n",
            "[51 | 118.78] loss=2.99 avg=3.09\n",
            "[52 | 121.10] loss=2.82 avg=3.09\n",
            "[53 | 123.42] loss=2.99 avg=3.08\n",
            "[54 | 125.73] loss=2.99 avg=3.08\n",
            "[55 | 128.03] loss=2.91 avg=3.08\n",
            "[56 | 130.34] loss=3.02 avg=3.08\n",
            "[57 | 132.63] loss=2.92 avg=3.07\n",
            "[58 | 134.92] loss=2.84 avg=3.07\n",
            "[59 | 137.22] loss=2.93 avg=3.06\n",
            "[60 | 139.51] loss=3.00 avg=3.06\n",
            "[61 | 141.80] loss=3.01 avg=3.06\n",
            "[62 | 144.08] loss=2.88 avg=3.06\n",
            "[63 | 146.36] loss=2.94 avg=3.05\n",
            "[64 | 148.65] loss=2.82 avg=3.05\n",
            "[65 | 150.93] loss=2.86 avg=3.05\n",
            "[66 | 153.22] loss=3.02 avg=3.05\n",
            "[67 | 155.52] loss=3.04 avg=3.05\n",
            "[68 | 157.82] loss=2.78 avg=3.04\n",
            "[69 | 160.12] loss=2.87 avg=3.04\n",
            "[70 | 162.42] loss=2.93 avg=3.03\n",
            "[71 | 164.75] loss=3.04 avg=3.03\n",
            "[72 | 167.06] loss=3.06 avg=3.03\n",
            "[73 | 169.39] loss=2.92 avg=3.03\n",
            "[74 | 171.71] loss=2.89 avg=3.03\n",
            "[75 | 174.05] loss=2.77 avg=3.03\n",
            "[76 | 176.39] loss=2.86 avg=3.02\n",
            "[77 | 178.73] loss=2.68 avg=3.02\n",
            "[78 | 181.08] loss=2.83 avg=3.01\n",
            "[79 | 183.41] loss=2.83 avg=3.01\n",
            "[80 | 185.76] loss=3.03 avg=3.01\n",
            "[81 | 188.11] loss=2.88 avg=3.01\n",
            "[82 | 190.46] loss=2.81 avg=3.00\n",
            "[83 | 192.80] loss=2.94 avg=3.00\n",
            "[84 | 195.13] loss=2.71 avg=3.00\n",
            "[85 | 197.46] loss=2.78 avg=2.99\n",
            "[86 | 199.78] loss=2.89 avg=2.99\n",
            "[87 | 202.12] loss=2.83 avg=2.99\n",
            "[88 | 204.44] loss=2.96 avg=2.99\n",
            "[89 | 206.75] loss=2.98 avg=2.99\n",
            "[90 | 209.08] loss=2.88 avg=2.99\n",
            "[91 | 211.39] loss=2.86 avg=2.98\n",
            "[92 | 213.72] loss=2.90 avg=2.98\n",
            "[93 | 216.04] loss=2.72 avg=2.98\n",
            "[94 | 218.35] loss=2.68 avg=2.97\n",
            "[95 | 220.66] loss=2.91 avg=2.97\n",
            "[96 | 222.97] loss=2.84 avg=2.97\n",
            "[97 | 225.27] loss=2.99 avg=2.97\n",
            "[98 | 227.59] loss=2.64 avg=2.97\n",
            "[99 | 229.89] loss=2.83 avg=2.96\n",
            "[100 | 232.20] loss=2.88 avg=2.96\n",
            "======== SAMPLE 1 ========\n",
            " him his father's house, and then the boy came down at my door and said, 'What is it?\" My father said, \"Nothing, boy. No, you will not hurt me.\" \"What is it?\" The boy shouted, but they were all in black, the boys all in black. \"We had no choice, or we would be dead. I said, 'No, you are to lie down, boy. Go back to the wall. Make the wall round, and bring the door up. Let your father in to make the wall round.\" \n",
            "Bran was still seated at his table, his hands clenched behind his back. As they sat, the children laughed. Their faces moved with the same gentle and mocking pace of a man in the dusk, breathing the air like \n",
            "pig, who did not seem afraid of anything, even the ground. The children said things they would never \n",
            "say: Jafar's mother had been a woman to him, and his father had been one of his sisters. He could not remember the last time he \n",
            "had been frightened, but the one last time, the night he had been taken with the children. He did not think he would feel afraid until \n",
            "he had left the room; when he crossed the room, a door opened and men in black cloaks came in. \n",
            "Tyrion Lannister's voice could not be heard from him, but he smiled, and all eyes were on him. \n",
            "By the time the children finally left the room, the children were done with him. He smiled and laughed and made the \n",
            "smiles look as handsome as a man could wear. The laughter was louder when his mouth was open, and they began a \n",
            "slow ride back to Winterfell. The journey took no more than ten hours; the girls slept at his side and \n",
            "went to the Vale with Tyrion and his brother Catelyn, and the boys went across the Trident to wed Tywin. \n",
            "His daughter was taken by her father and buried at Rhoyne, the House of Arryn. \n",
            "She had been born in the Wall. She never had a father before, so their vows were as sacred as children's names, \n",
            "but they were children now, sons and daughters, sisters and brothers, sons and sisters and brothers. \n",
            "At the very end of the journey, when Tyrion had finished eating, he went down into the crowd to look in \n",
            "the face of \n",
            "Gare. They were there, he told himself. They were the women of Joffrey, but they were not women as \n",
            "they had been born. They would not be like that at all. He liked them all, he said. One night, when \n",
            "they went to hunt, they put on a brave white cloak and rode along the Trident in front of the \n",
            "kingly temple . . . with him. \n",
            "He climbed a tree, and he saw the seven towers looming in the distance, and Tyrion heard voices in the stone, as they \n",
            "Page 281\n",
            "\n",
            "raided around him, saying, \"The gods are like this, and as gods you must not hate.\" The faces of the women on the \n",
            "crests, the faces of the children, even the faces of men were as gods as any woman in life \n",
            "would wish. \n",
            "\"You are to be a bannerman, my lord, for the banner,\" Tyrion told them. \"I will tell you what your vows mean. I hope your \n",
            "blood flows this way through the tower, and you will be as brave in the tower as I am inside. All things shall happen as they come \n",
            "through the fire.\" \n",
            "\"All things shall happen, my lord!\" \n",
            "\"The truth is that all things are as they come.\" \n",
            "\"The men shall be sent down to the throne room. The women with wings of flame. The children, those who are \n",
            "blessed, the gods, those chosen, and the rest shall die in the tower as the gods die in the fire.\" \n",
            "Tyrion did as they told him, and the girls sat around him. The children were singing, but the \n",
            "people were \n",
            "not. Tyrion knew they were singers as well as men, but that meant they talked loudly as \n",
            "you can hear them talk. The word was \n",
            "burden. \n",
            "As the boys took their places in each other's cups, Tyrion heard the voices of men's wives and daughters, \n",
            "and the names of the others. He could hear the women singing, but they were not his, and they never \n",
            "him. \n",
            "The girls went up beside him in his cups, and he heard them sing. The women were his eyes, and they were his ears, \n",
            "and he could hear the voices, but the songs were words. The woman's voice was \n",
            "\n",
            "\n",
            "[101 | 246.42] loss=2.80 avg=2.96\n",
            "[102 | 248.75] loss=2.85 avg=2.96\n",
            "[103 | 251.08] loss=2.93 avg=2.96\n",
            "[104 | 253.40] loss=2.84 avg=2.96\n",
            "[105 | 255.72] loss=2.78 avg=2.95\n",
            "[106 | 258.05] loss=2.71 avg=2.95\n",
            "[107 | 260.38] loss=2.74 avg=2.95\n",
            "[108 | 262.71] loss=2.83 avg=2.94\n",
            "[109 | 265.04] loss=2.79 avg=2.94\n",
            "[110 | 267.36] loss=2.63 avg=2.94\n",
            "[111 | 269.70] loss=2.76 avg=2.93\n",
            "[112 | 272.04] loss=2.81 avg=2.93\n",
            "[113 | 274.37] loss=2.86 avg=2.93\n",
            "[114 | 276.71] loss=2.81 avg=2.93\n",
            "[115 | 279.04] loss=2.65 avg=2.93\n",
            "[116 | 281.38] loss=2.80 avg=2.92\n",
            "[117 | 283.70] loss=2.76 avg=2.92\n",
            "[118 | 286.04] loss=2.84 avg=2.92\n",
            "[119 | 288.36] loss=2.76 avg=2.92\n",
            "[120 | 290.69] loss=2.73 avg=2.92\n",
            "[121 | 293.01] loss=2.70 avg=2.91\n",
            "[122 | 295.34] loss=2.79 avg=2.91\n",
            "[123 | 297.66] loss=2.91 avg=2.91\n",
            "[124 | 299.98] loss=2.66 avg=2.91\n",
            "[125 | 302.31] loss=2.72 avg=2.90\n",
            "[126 | 304.62] loss=2.55 avg=2.90\n",
            "[127 | 306.94] loss=2.70 avg=2.90\n",
            "[128 | 309.26] loss=2.70 avg=2.89\n",
            "[129 | 311.59] loss=2.65 avg=2.89\n",
            "[130 | 313.90] loss=2.74 avg=2.89\n",
            "[131 | 316.23] loss=2.65 avg=2.89\n",
            "[132 | 318.54] loss=2.72 avg=2.88\n",
            "[133 | 320.86] loss=2.74 avg=2.88\n",
            "[134 | 323.18] loss=2.74 avg=2.88\n",
            "[135 | 325.50] loss=2.78 avg=2.88\n",
            "[136 | 327.81] loss=2.79 avg=2.88\n",
            "[137 | 330.12] loss=2.69 avg=2.87\n",
            "[138 | 332.44] loss=2.66 avg=2.87\n",
            "[139 | 334.76] loss=2.67 avg=2.87\n",
            "[140 | 337.08] loss=2.63 avg=2.87\n",
            "[141 | 339.40] loss=2.79 avg=2.86\n",
            "[142 | 341.72] loss=2.54 avg=2.86\n",
            "[143 | 344.05] loss=2.75 avg=2.86\n",
            "[144 | 346.37] loss=2.78 avg=2.86\n",
            "[145 | 348.70] loss=2.81 avg=2.86\n",
            "[146 | 351.03] loss=2.49 avg=2.85\n",
            "[147 | 353.35] loss=2.68 avg=2.85\n",
            "[148 | 355.67] loss=2.73 avg=2.85\n",
            "[149 | 358.00] loss=2.77 avg=2.85\n",
            "[150 | 360.34] loss=2.54 avg=2.84\n",
            "[151 | 362.67] loss=2.73 avg=2.84\n",
            "[152 | 365.01] loss=2.69 avg=2.84\n",
            "[153 | 367.34] loss=2.65 avg=2.84\n",
            "[154 | 369.67] loss=2.74 avg=2.84\n",
            "[155 | 372.02] loss=2.62 avg=2.83\n",
            "[156 | 374.35] loss=2.65 avg=2.83\n",
            "[157 | 376.70] loss=2.78 avg=2.83\n",
            "[158 | 379.03] loss=2.56 avg=2.83\n",
            "[159 | 381.36] loss=2.56 avg=2.82\n",
            "[160 | 383.69] loss=2.49 avg=2.82\n",
            "[161 | 386.04] loss=2.70 avg=2.82\n",
            "[162 | 388.38] loss=2.57 avg=2.82\n",
            "[163 | 390.72] loss=2.76 avg=2.81\n",
            "[164 | 393.05] loss=2.46 avg=2.81\n",
            "[165 | 395.39] loss=2.49 avg=2.81\n",
            "[166 | 397.73] loss=2.78 avg=2.81\n",
            "[167 | 400.06] loss=2.64 avg=2.80\n",
            "[168 | 402.39] loss=2.69 avg=2.80\n",
            "[169 | 404.71] loss=2.47 avg=2.80\n",
            "[170 | 407.05] loss=2.61 avg=2.80\n",
            "[171 | 409.39] loss=2.85 avg=2.80\n",
            "[172 | 411.72] loss=2.82 avg=2.80\n",
            "[173 | 414.05] loss=2.78 avg=2.80\n",
            "[174 | 416.37] loss=2.61 avg=2.79\n",
            "[175 | 418.70] loss=2.63 avg=2.79\n",
            "[176 | 421.04] loss=2.58 avg=2.79\n",
            "[177 | 423.36] loss=2.60 avg=2.79\n",
            "[178 | 425.69] loss=2.74 avg=2.79\n",
            "[179 | 428.01] loss=2.54 avg=2.78\n",
            "[180 | 430.34] loss=2.59 avg=2.78\n",
            "[181 | 432.68] loss=2.65 avg=2.78\n",
            "[182 | 435.00] loss=2.71 avg=2.78\n",
            "[183 | 437.33] loss=2.56 avg=2.78\n",
            "[184 | 439.65] loss=2.72 avg=2.78\n",
            "[185 | 441.98] loss=2.67 avg=2.77\n",
            "[186 | 444.32] loss=2.68 avg=2.77\n",
            "[187 | 446.63] loss=2.55 avg=2.77\n",
            "[188 | 448.97] loss=2.64 avg=2.77\n",
            "[189 | 451.29] loss=2.61 avg=2.77\n",
            "[190 | 453.61] loss=2.63 avg=2.77\n",
            "[191 | 455.94] loss=2.57 avg=2.76\n",
            "[192 | 458.27] loss=2.66 avg=2.76\n",
            "[193 | 460.59] loss=2.70 avg=2.76\n",
            "[194 | 462.91] loss=2.38 avg=2.76\n",
            "[195 | 465.24] loss=2.45 avg=2.75\n",
            "[196 | 467.56] loss=2.52 avg=2.75\n",
            "[197 | 469.89] loss=2.60 avg=2.75\n",
            "[198 | 472.21] loss=2.40 avg=2.75\n",
            "[199 | 474.54] loss=2.61 avg=2.74\n",
            "[200 | 476.86] loss=2.45 avg=2.74\n",
            "======== SAMPLE 1 ========\n",
            " Hand. \"There's no way we'll ever be here.\" \n",
            "\"We'll be, all of us,\" said Joffrey. \n",
            "\"And then there will be Winterfell,\" he said gravely, \"and you'll find we don't know everything.\" \n",
            "\"I will, I promise,\" said Ser Jorah, and they left together, with the sun on their faces and Father and \n",
            "the Red Keep ahead of them, the sky a shadowless sky. The Greatjon stood before them all, black and \n",
            "white as a shield, his face blue as a shadow. His sword was in the air. The Seven \n",
            "Page 169\n",
            "\n",
            "stars burned in the sky, and Father watched silently as Ser Jorah rose and walked down the hall, a long \n",
            "sept, a hundred years to the day. He talked to all the men in the king's court, but his eyes wandered to his own \n",
            "wrights. He was so handsome. He wore only a high plate of gold and iron, and a crown made of \n",
            "golden leaves, golden as ever. \n",
            "\"I wonder if Lord Eddard would like to show me his sword again, Joffrey,\" he said in a \n",
            "loud whisper that was almost as sad as the silence for Catelyn Stark. \n",
            "Catelyn Stark did not think so. She stood there with Joffrey's tears on the sky, with her head \n",
            "down. She listened to Ser Jorah play with his wits. \n",
            "Finally Father asked if Ser Jorah would go with him. \n",
            "Catelyn Stark nodded. \"It seems to me that Lord Stannis is coming, with my brothers,\" she said. \n",
            "The king was in the east, she knew. They seemed to be arguing together at least a dozen times a day. If \n",
            "Stannis were to visit, the Stark children would need a king to battle. When she heard the old talk about the Dornish \n",
            "king, she turned away from her bedside and saw Father standing in front of the window, with two heavy \n",
            "guardsmen in his arms. Catelyn Stark did not think of him as a king. If anything, he seemed less than \n",
            "brother. \n",
            "But she did think of him as Lady Stark, and perhaps more than any of them. They were his people, after all. \n",
            "Lord Jason Tyrell was the reigning king at the time, but he had died a few years previous, \n",
            "and Catelyn Stark found herself thinking the words of his own words as she spoke them. \n",
            "Something in her was wrong, perhaps, but Lord Eddard was a strong man, tall and strong. \n",
            "When Catelyn Stark heard that, Ser Jorah Mormont was with her. The king's guardsmen had taken them into \n",
            "the king's court, and even brought them before the king himself. They had not been allowed to leave the \n",
            "court rooms until Ser Jorah had returned, and had even refused to let them enter the \n",
            "room where Lord Eddard had been slain, even if it meant facing away from his throne. \n",
            "\"I am sorry, Lady Stark . . .\" \n",
            "\"As if they hadn't been here all night. I suppose I am just a bit tired myself now, at least . . . I wonder how things will be-\" \n",
            "\"I am not so tired, either.\" His face was flushed as his hands shook and the words broke free, but they \n",
            "were still there. \"I . . . I suppose I'd ask you once or twice if you'd like to come with me to the \n",
            "Klingon Fists.\" \n",
            "Page 170\n",
            "\n",
            "The king's court was so far from the high seat in their kingly court that it was hard to tell what to \n",
            "shame Catelyn Stark. But that did change on the last day of the tourney when she found herself \n",
            "there. The queen had told her brother Jaime the truth, of course, but no more. She would be \n",
            "granting him a tourney right from now, the way the Caster's Seal had been in her brother's memory. \n",
            "\"I have no notion,\" she said. \"It is so strange. I am Robert Stark. How long can it be until we reach Winterfell?\" \n",
            "\"Your son is with us all the same day, as always. I am only a boy.\" \n",
            "\"Why would I stay in the Seven Kingdoms longer? I can go straight to the Eyrie and speak with Lord Commander \n",
            "Catelyn?\" \n",
            "\"He will not have the time of his own.\" \n",
            "Catelyn Stark did not know why. Was the king afraid? She had thought so. She had wondered what might \n",
            "have happened to her if Cersei had come to King's Landing with her\n",
            "\n",
            "[201 | 490.11] loss=2.56 avg=2.74\n",
            "[202 | 492.43] loss=2.67 avg=2.74\n",
            "[203 | 494.75] loss=2.59 avg=2.74\n",
            "[204 | 497.08] loss=2.49 avg=2.73\n",
            "[205 | 499.40] loss=2.38 avg=2.73\n",
            "[206 | 501.72] loss=2.51 avg=2.73\n",
            "[207 | 504.04] loss=2.48 avg=2.72\n",
            "[208 | 506.36] loss=2.66 avg=2.72\n",
            "[209 | 508.68] loss=2.47 avg=2.72\n",
            "[210 | 510.99] loss=2.62 avg=2.72\n",
            "[211 | 513.32] loss=2.41 avg=2.72\n",
            "[212 | 515.63] loss=2.52 avg=2.71\n",
            "[213 | 517.95] loss=2.53 avg=2.71\n",
            "[214 | 520.28] loss=2.55 avg=2.71\n",
            "[215 | 522.59] loss=2.49 avg=2.71\n",
            "[216 | 524.91] loss=2.46 avg=2.70\n",
            "[217 | 527.23] loss=2.45 avg=2.70\n",
            "[218 | 529.55] loss=2.42 avg=2.70\n",
            "[219 | 531.86] loss=2.63 avg=2.70\n",
            "[220 | 534.18] loss=2.38 avg=2.69\n",
            "[221 | 536.49] loss=2.59 avg=2.69\n",
            "[222 | 538.81] loss=2.56 avg=2.69\n",
            "[223 | 541.14] loss=2.75 avg=2.69\n",
            "[224 | 543.48] loss=2.58 avg=2.69\n",
            "[225 | 545.80] loss=2.43 avg=2.69\n",
            "[226 | 548.12] loss=2.62 avg=2.69\n",
            "[227 | 550.44] loss=2.43 avg=2.68\n",
            "[228 | 552.76] loss=2.48 avg=2.68\n",
            "[229 | 555.08] loss=2.50 avg=2.68\n",
            "[230 | 557.40] loss=2.54 avg=2.68\n",
            "[231 | 559.73] loss=2.54 avg=2.68\n",
            "[232 | 562.05] loss=2.47 avg=2.67\n",
            "[233 | 564.37] loss=2.67 avg=2.67\n",
            "[234 | 566.70] loss=2.39 avg=2.67\n",
            "[235 | 569.03] loss=2.52 avg=2.67\n",
            "[236 | 571.34] loss=2.33 avg=2.67\n",
            "[237 | 573.67] loss=2.49 avg=2.66\n",
            "[238 | 575.98] loss=2.66 avg=2.66\n",
            "[239 | 578.31] loss=2.43 avg=2.66\n",
            "[240 | 580.64] loss=2.49 avg=2.66\n",
            "[241 | 582.96] loss=2.37 avg=2.66\n",
            "[242 | 585.29] loss=2.46 avg=2.65\n",
            "[243 | 587.60] loss=2.42 avg=2.65\n",
            "[244 | 589.93] loss=2.51 avg=2.65\n",
            "[245 | 592.26] loss=2.48 avg=2.65\n",
            "[246 | 594.58] loss=2.32 avg=2.64\n",
            "[247 | 596.90] loss=2.27 avg=2.64\n",
            "[248 | 599.22] loss=2.60 avg=2.64\n",
            "[249 | 601.54] loss=2.51 avg=2.64\n",
            "[250 | 603.86] loss=2.56 avg=2.64\n",
            "[251 | 606.20] loss=2.50 avg=2.64\n",
            "[252 | 608.52] loss=2.41 avg=2.63\n",
            "[253 | 610.85] loss=2.48 avg=2.63\n",
            "[254 | 613.16] loss=2.57 avg=2.63\n",
            "[255 | 615.49] loss=2.49 avg=2.63\n",
            "[256 | 617.83] loss=2.31 avg=2.63\n",
            "[257 | 620.14] loss=2.38 avg=2.62\n",
            "[258 | 622.46] loss=2.37 avg=2.62\n",
            "[259 | 624.78] loss=2.62 avg=2.62\n",
            "[260 | 627.10] loss=2.33 avg=2.62\n",
            "[261 | 629.44] loss=2.29 avg=2.61\n",
            "[262 | 631.77] loss=2.45 avg=2.61\n",
            "[263 | 634.10] loss=2.33 avg=2.61\n",
            "[264 | 636.43] loss=2.44 avg=2.61\n",
            "[265 | 638.74] loss=2.55 avg=2.61\n",
            "[266 | 641.08] loss=2.42 avg=2.61\n",
            "[267 | 643.41] loss=2.40 avg=2.60\n",
            "[268 | 645.74] loss=2.28 avg=2.60\n",
            "[269 | 648.06] loss=2.41 avg=2.60\n",
            "[270 | 650.39] loss=2.44 avg=2.60\n",
            "[271 | 652.72] loss=2.14 avg=2.59\n",
            "[272 | 655.05] loss=2.20 avg=2.59\n",
            "[273 | 657.37] loss=2.24 avg=2.58\n",
            "[274 | 659.70] loss=2.55 avg=2.58\n",
            "[275 | 662.02] loss=2.56 avg=2.58\n",
            "[276 | 664.35] loss=2.21 avg=2.58\n",
            "[277 | 666.68] loss=2.59 avg=2.58\n",
            "[278 | 669.01] loss=2.32 avg=2.58\n",
            "[279 | 671.32] loss=2.18 avg=2.57\n",
            "[280 | 673.65] loss=2.34 avg=2.57\n",
            "[281 | 675.98] loss=2.37 avg=2.57\n",
            "[282 | 678.31] loss=2.47 avg=2.57\n",
            "[283 | 680.65] loss=2.46 avg=2.57\n",
            "[284 | 682.97] loss=2.21 avg=2.56\n",
            "[285 | 685.30] loss=2.29 avg=2.56\n",
            "[286 | 687.62] loss=2.37 avg=2.56\n",
            "[287 | 689.94] loss=2.12 avg=2.55\n",
            "[288 | 692.26] loss=2.49 avg=2.55\n",
            "[289 | 694.59] loss=2.43 avg=2.55\n",
            "[290 | 696.92] loss=2.22 avg=2.55\n",
            "[291 | 699.24] loss=2.32 avg=2.54\n",
            "[292 | 701.58] loss=2.18 avg=2.54\n",
            "[293 | 703.91] loss=2.05 avg=2.53\n",
            "[294 | 706.23] loss=2.18 avg=2.53\n",
            "[295 | 708.56] loss=2.41 avg=2.53\n",
            "[296 | 710.89] loss=2.28 avg=2.53\n",
            "[297 | 713.22] loss=2.23 avg=2.52\n",
            "[298 | 715.56] loss=2.10 avg=2.52\n",
            "[299 | 717.89] loss=2.13 avg=2.52\n",
            "[300 | 720.22] loss=2.38 avg=2.51\n",
            "======== SAMPLE 1 ========\n",
            ",\" Lord Stark \n",
            "Page 438\n",
            "\n",
            "was a wise man, and his brother Lord Baratheon was a true man as well. If he fell, the \n",
            "Page 439\n",
            "\n",
            "lords would gladly sell him, he knew the words and the meanings of them. Ned \n",
            "thought he might find a few friends amongst the old men. The young man was a quiet man, \n",
            "bubbly, but brave enough to stand his ground without a shield or a belt around his head. The \n",
            "young man did not even wear armor; his hair was plastered with gold and silver, and his cloak was made of steel \n",
            "and \n",
            "sapphara, sometimes black. He was quick to talk his way out from beneath the parapet, and the rest \n",
            "were easy as walking links. He was good-looking too, but he seemed to be wearing a leather \n",
            "corduroys high enough to look at. \n",
            "Page 440\n",
            "\n",
            "And Robert? That was the hard part; he was forty, hard a thousand yards, a man who wore armor, shield, \n",
            "and spear and shield, and the armor he wore was dark grey leather, festooned with silver bands. He \n",
            "looked at Jon a hundred more times before he said, \"Little man, you look like a fool. \n",
            "The Kingslayer wants war, m'lord. Tell them and me and we will get it as soon as it is \n",
            "enough.\" \n",
            "The old man looked at him, and his face was as blue as dawn. \"Are you certain? I'm certain. The \n",
            "Kingslayer has everything. He will not take your life. And what he will not take your life is your oath. I \n",
            "will swear to you, that I will keep the king's peace, that I will keep the king. I promise, I swear to \n",
            "you. And now, the man's taken my oath. We shall not let you off the hook.\" \n",
            "The old man held up his hands. \"Keep the king's peace is the heart of the oath. It \n",
            "contains the truth we hold as brothers. Promise me, that oath, and I swear to you, that's the truth \n",
            "I want.\" He lifted his left hand. He seemed to move at a hundred feet a second. \n",
            "Jon broke off onto the yard and put his right one between his eyes, and as he was about to throw \n",
            "the first stone, he heard the soft clang of stone. Jon saw it grow, and saw the blackness beneath it, as \n",
            "a great wall. All the way down the back of the man's body, through his feet, the walls of the \n",
            "Dorne and the First Men put out a chill. \n",
            "His voice was cold and dead. He saw what had happened to Rickard, and how the septon had \n",
            "turned on him, how the man's eyes had shut, and the tears ran down his face. \n",
            "The Dothraki were screaming, and the man went to the ground. \n",
            "He saw the bones of Robb, the man on his head. He saw half his eyes, the one half open \n",
            "and his face as huge and fat as the last of winter. \n",
            "'Put my foot down, boy, put my foot down, put my foot down. What's wrong, Robert?\" \n",
            "The Lord Commander leaned forward and lifted his left shoulder. The grey stain of rain \n",
            "had trickled down his leg. \"My lord, the blood is running down my leg. Do not cry it out.\" \n",
            "'The man is dead by now. No bloodstains can be found \n",
            "there. The river stinks of blood. I need to drink what I can out of the dead man.\" \n",
            "'You are sick of my stories. Do you think you can make me die by pretending that the dead man is not \n",
            "a dead man? I want to feel the pain, I want to feel the pain, I want to feel the pain. I want to \n",
            "be the one standing before the king. How do you know that? How do you know that? Your \n",
            "father was the first to die, Your father was the first to die . . . He drank the man's blood, \n",
            "then burned the man, and the king and the king's blood. What does that tell you, Your \n",
            "Father? I can't imagine that. It scares me to tears. Do you want to say they burned the \n",
            "man in the street, or the man in the wood? That must be the man in the trees. Tell me.\" \n",
            "'The wood . . . the real wood . . . the dark stone . . . the dark stone . . . tell me. \n",
            "The man in the tree was dead before my eyes, dead with his head . . . and the\n",
            "\n",
            "[301 | 733.31] loss=2.22 avg=2.51\n",
            "[302 | 735.64] loss=2.16 avg=2.51\n",
            "[303 | 737.97] loss=2.19 avg=2.50\n",
            "[304 | 740.30] loss=2.29 avg=2.50\n",
            "[305 | 742.62] loss=2.29 avg=2.50\n",
            "[306 | 744.95] loss=2.42 avg=2.50\n",
            "[307 | 747.28] loss=2.41 avg=2.50\n",
            "[308 | 749.60] loss=2.49 avg=2.50\n",
            "[309 | 751.93] loss=2.22 avg=2.49\n",
            "[310 | 754.25] loss=2.08 avg=2.49\n",
            "[311 | 756.57] loss=2.27 avg=2.49\n",
            "[312 | 758.90] loss=2.33 avg=2.49\n",
            "[313 | 761.23] loss=2.21 avg=2.48\n",
            "[314 | 763.56] loss=2.30 avg=2.48\n",
            "[315 | 765.87] loss=2.31 avg=2.48\n",
            "[316 | 768.19] loss=2.12 avg=2.48\n",
            "[317 | 770.51] loss=2.10 avg=2.47\n",
            "[318 | 772.83] loss=1.99 avg=2.47\n",
            "[319 | 775.16] loss=2.21 avg=2.46\n",
            "[320 | 777.47] loss=2.52 avg=2.47\n",
            "[321 | 779.79] loss=2.42 avg=2.46\n",
            "[322 | 782.11] loss=2.15 avg=2.46\n",
            "[323 | 784.44] loss=2.31 avg=2.46\n",
            "[324 | 786.76] loss=2.03 avg=2.46\n",
            "[325 | 789.08] loss=2.39 avg=2.45\n",
            "[326 | 791.40] loss=2.22 avg=2.45\n",
            "[327 | 793.71] loss=2.29 avg=2.45\n",
            "[328 | 796.04] loss=2.29 avg=2.45\n",
            "[329 | 798.37] loss=2.24 avg=2.45\n",
            "[330 | 800.69] loss=2.18 avg=2.44\n",
            "[331 | 803.00] loss=2.42 avg=2.44\n",
            "[332 | 805.32] loss=2.44 avg=2.44\n",
            "[333 | 807.63] loss=2.19 avg=2.44\n",
            "[334 | 809.95] loss=2.09 avg=2.44\n",
            "[335 | 812.27] loss=2.12 avg=2.43\n",
            "[336 | 814.58] loss=1.83 avg=2.43\n",
            "[337 | 816.91] loss=2.19 avg=2.43\n",
            "[338 | 819.22] loss=2.02 avg=2.42\n",
            "[339 | 821.54] loss=2.17 avg=2.42\n",
            "[340 | 823.86] loss=2.27 avg=2.42\n",
            "[341 | 826.18] loss=2.06 avg=2.41\n",
            "[342 | 828.50] loss=2.00 avg=2.41\n",
            "[343 | 830.82] loss=2.17 avg=2.41\n",
            "[344 | 833.14] loss=2.05 avg=2.40\n",
            "[345 | 835.47] loss=1.95 avg=2.40\n",
            "[346 | 837.79] loss=2.32 avg=2.40\n",
            "[347 | 840.12] loss=2.17 avg=2.39\n",
            "[348 | 842.44] loss=2.47 avg=2.40\n",
            "[349 | 844.76] loss=2.44 avg=2.40\n",
            "[350 | 847.09] loss=2.34 avg=2.40\n",
            "[351 | 849.41] loss=2.11 avg=2.39\n",
            "[352 | 851.73] loss=2.31 avg=2.39\n",
            "[353 | 854.05] loss=2.22 avg=2.39\n",
            "[354 | 856.38] loss=2.05 avg=2.39\n",
            "[355 | 858.71] loss=1.91 avg=2.38\n",
            "[356 | 861.03] loss=2.37 avg=2.38\n",
            "[357 | 863.36] loss=2.07 avg=2.38\n",
            "[358 | 865.68] loss=2.18 avg=2.38\n",
            "[359 | 868.00] loss=2.18 avg=2.37\n",
            "[360 | 870.33] loss=2.41 avg=2.37\n",
            "[361 | 872.67] loss=2.15 avg=2.37\n",
            "[362 | 874.99] loss=2.09 avg=2.37\n",
            "[363 | 877.32] loss=2.34 avg=2.37\n",
            "[364 | 879.64] loss=2.15 avg=2.37\n",
            "[365 | 881.97] loss=1.87 avg=2.36\n",
            "[366 | 884.32] loss=2.19 avg=2.36\n",
            "[367 | 886.64] loss=2.13 avg=2.36\n",
            "[368 | 888.97] loss=2.04 avg=2.35\n",
            "[369 | 891.28] loss=2.06 avg=2.35\n",
            "[370 | 893.62] loss=2.09 avg=2.35\n",
            "[371 | 895.96] loss=2.16 avg=2.35\n",
            "[372 | 898.28] loss=2.18 avg=2.35\n",
            "[373 | 900.61] loss=2.28 avg=2.34\n",
            "[374 | 902.94] loss=2.08 avg=2.34\n",
            "[375 | 905.26] loss=2.30 avg=2.34\n",
            "[376 | 907.60] loss=2.16 avg=2.34\n",
            "[377 | 909.93] loss=2.05 avg=2.34\n",
            "[378 | 912.26] loss=2.14 avg=2.33\n",
            "[379 | 914.58] loss=1.96 avg=2.33\n",
            "[380 | 916.91] loss=2.06 avg=2.33\n",
            "[381 | 919.23] loss=2.29 avg=2.33\n",
            "[382 | 921.57] loss=1.93 avg=2.32\n",
            "[383 | 923.89] loss=2.19 avg=2.32\n",
            "[384 | 926.22] loss=1.91 avg=2.32\n",
            "[385 | 928.54] loss=1.93 avg=2.31\n",
            "[386 | 930.88] loss=2.19 avg=2.31\n",
            "[387 | 933.21] loss=1.90 avg=2.31\n",
            "[388 | 935.54] loss=1.84 avg=2.30\n",
            "[389 | 937.86] loss=1.98 avg=2.30\n",
            "[390 | 940.19] loss=2.06 avg=2.30\n",
            "[391 | 942.52] loss=2.12 avg=2.30\n",
            "[392 | 944.85] loss=2.01 avg=2.29\n",
            "[393 | 947.18] loss=1.88 avg=2.29\n",
            "[394 | 949.50] loss=1.90 avg=2.29\n",
            "[395 | 951.83] loss=2.04 avg=2.28\n",
            "[396 | 954.16] loss=2.19 avg=2.28\n",
            "[397 | 956.49] loss=1.99 avg=2.28\n",
            "[398 | 958.83] loss=2.03 avg=2.28\n",
            "[399 | 961.17] loss=1.84 avg=2.27\n",
            "[400 | 963.49] loss=1.95 avg=2.27\n",
            "======== SAMPLE 1 ========\n",
            "hhoyo, \n",
            "\"The one who sees all, oh, oh,\" the maegi said. The rest were looking up at them with faces from another world. \n",
            "Viserys was in the middle of it; the man who made the mistake, the Lord Commander of the Night's \n",
            "Watch, the maester he had sent out so you sent me, Lord Tywin Lannister. \n",
            "\"I'll be damned,\" he muttered as Viserys moved from the window hatch to rise in a high girth above a small \n",
            "temple on the slope of the Trident. His cloak was on the ground, the gold cloaks he'd helped \n",
            "wrap around him as he walked with a long line. \"The king is not here!\" \n",
            "\"I have,\" Ser Jorah said as a thousand knights began to chant, \"Come to me once more and ask him to return!\" \n",
            "Page 422\n",
            "\n",
            "There was only one way to get out: to the king himself. \n",
            "Ser Jorah Mormont was on his knees beside him in the godswood. \"Rhaego will see you,\" he said in a \n",
            "gentle, yet gravelly voice. His leg was hurting. \"My king, I must go. I cannot walk.\" \n",
            "Viserys was sitting in the tent when Viserrey emerged from the bowels of the Red Keep. The godswood \n",
            "was wide and dark, and yet even from the edge of the world, beneath the earth's crust, there was something different \n",
            "about him, something huge and dark and serene. Viserys swam toward him, a huge hand on his shoulder. \n",
            "Viserys speaks from experience. When you're inside the solar system, your eyes see. So long as you're in a big red \n",
            "tent, you have an immense sense of sight. This was no other place, this was the solar system by any \n",
            "tent's standard. Dany could scarcely see beyond the walls of the solar system, because there were no windows . . . but when \n",
            "she turned her eyes back inside out, Dany felt the eyes of the seven gods, seated on a vast plain \n",
            "that stretched deep into the black. She could see their faces; they were all seated facing the sun, at the top of the \n",
            "thousand-foot (1,400 meters) tall Sunburst. All seven of them stood side by side in a vast purple canopy, \n",
            "laying their gaze on each other, looking up at the sun from a distance like a giant's eyes. \n",
            "Viserys gestured. The godswood was in the process of flowing down from above. His hand had been \n",
            "laying heavily on the huge white animal's side, under the waters of the river. The black wings of the \n",
            "prancing lion were visible beneath the black cloth, and huge eyes stared at each other instead of away. \n",
            "Lysa's mother came out behind her, and Sansa ran past. A moment later the water crested the \n",
            "palisades and spread out below her, and the moonlight seemed to shine on the dawn sea. When the \n",
            "godswood rose above her, Sansa felt as if she were swimming in a great pool. It was almost as though she had never \n",
            "sailed before. There was no smell of the earth, no taste of it, just endless blue-chased waters, \n",
            "melt-filled with moss, and the earth itself was so alive with smells that only the flies would seem \n",
            "to smell it. She looked out over rooftops, down rivers, into the night cities of the Tyrells; into \n",
            "the shadow of Lannisport and King's Landing. She saw the blue lights in the windows of rooms in \n",
            "the houses of the Kingsguard; the faces of the other Kingsguard; the men, women, and children who had taken shelter within \n",
            "these walls; and no matter how hard Viserys forced himself to speak, no matter how much Sansa wanted to talk \n",
            "about the night of Dragon's Dawn, no matter how much she wanted to tell her mother that she had fled the \n",
            "City of the Kings, or the Prince of Wales; for whatever reason, her uncle Jaime had gone to the \n",
            "Kingsguard in order to wed a girl of thirty-one, a noble woman from the south. \n",
            "The gods were looking down on her. When Dany watched Viserys sit cross-legged on the great \n",
            "raven, her heart began to beat faster. \n",
            "\"Look at the faces of the men in the walls. They're all in the same boat.\" She tried to \n",
            "look up. \n",
            "She was blind and speechless. She could not look up. \n",
            "\"Look at the faces,\" Ser Ilyn said. \"\n",
            "\n",
            "[401 | 976.69] loss=1.99 avg=2.27\n",
            "[402 | 979.01] loss=2.02 avg=2.26\n",
            "[403 | 981.35] loss=1.90 avg=2.26\n",
            "[404 | 983.67] loss=2.16 avg=2.26\n",
            "[405 | 985.99] loss=2.10 avg=2.26\n",
            "[406 | 988.31] loss=2.00 avg=2.25\n",
            "[407 | 990.62] loss=1.56 avg=2.25\n",
            "[408 | 992.95] loss=1.80 avg=2.24\n",
            "[409 | 995.26] loss=1.93 avg=2.24\n",
            "[410 | 997.57] loss=2.33 avg=2.24\n",
            "[411 | 999.89] loss=1.99 avg=2.24\n",
            "[412 | 1002.21] loss=1.81 avg=2.23\n",
            "[413 | 1004.52] loss=1.90 avg=2.23\n",
            "[414 | 1006.85] loss=2.28 avg=2.23\n",
            "[415 | 1009.16] loss=1.75 avg=2.23\n",
            "[416 | 1011.48] loss=1.77 avg=2.22\n",
            "[417 | 1013.79] loss=2.18 avg=2.22\n",
            "[418 | 1016.10] loss=2.22 avg=2.22\n",
            "[419 | 1018.43] loss=1.84 avg=2.22\n",
            "[420 | 1020.75] loss=2.16 avg=2.22\n",
            "[421 | 1023.07] loss=1.96 avg=2.21\n",
            "[422 | 1025.39] loss=1.83 avg=2.21\n",
            "[423 | 1027.71] loss=2.17 avg=2.21\n",
            "[424 | 1030.04] loss=2.38 avg=2.21\n",
            "[425 | 1032.37] loss=1.93 avg=2.21\n",
            "[426 | 1034.70] loss=1.84 avg=2.20\n",
            "[427 | 1037.02] loss=1.83 avg=2.20\n",
            "[428 | 1039.35] loss=1.77 avg=2.20\n",
            "[429 | 1041.69] loss=2.09 avg=2.20\n",
            "[430 | 1044.03] loss=2.06 avg=2.19\n",
            "[431 | 1046.36] loss=2.05 avg=2.19\n",
            "[432 | 1048.69] loss=2.29 avg=2.19\n",
            "[433 | 1051.03] loss=1.88 avg=2.19\n",
            "[434 | 1053.35] loss=1.89 avg=2.19\n",
            "[435 | 1055.70] loss=1.80 avg=2.18\n",
            "[436 | 1058.03] loss=1.75 avg=2.18\n",
            "[437 | 1060.36] loss=1.98 avg=2.18\n",
            "[438 | 1062.69] loss=2.02 avg=2.18\n",
            "[439 | 1065.02] loss=2.12 avg=2.17\n",
            "[440 | 1067.36] loss=2.16 avg=2.17\n",
            "[441 | 1069.69] loss=1.63 avg=2.17\n",
            "[442 | 1072.04] loss=1.72 avg=2.16\n",
            "[443 | 1074.37] loss=2.01 avg=2.16\n",
            "[444 | 1076.70] loss=1.70 avg=2.16\n",
            "[445 | 1079.05] loss=1.76 avg=2.15\n",
            "[446 | 1081.39] loss=1.96 avg=2.15\n",
            "[447 | 1083.72] loss=1.93 avg=2.15\n",
            "[448 | 1086.06] loss=1.85 avg=2.15\n",
            "[449 | 1088.39] loss=2.33 avg=2.15\n",
            "[450 | 1090.73] loss=2.17 avg=2.15\n",
            "[451 | 1093.08] loss=1.76 avg=2.14\n",
            "[452 | 1095.41] loss=1.75 avg=2.14\n",
            "[453 | 1097.75] loss=1.83 avg=2.14\n",
            "[454 | 1100.08] loss=2.02 avg=2.14\n",
            "[455 | 1102.41] loss=1.61 avg=2.13\n",
            "[456 | 1104.76] loss=1.82 avg=2.13\n",
            "[457 | 1107.10] loss=1.95 avg=2.13\n",
            "[458 | 1109.42] loss=1.35 avg=2.12\n",
            "[459 | 1111.74] loss=1.47 avg=2.11\n",
            "[460 | 1114.08] loss=1.91 avg=2.11\n",
            "[461 | 1116.42] loss=1.87 avg=2.11\n",
            "[462 | 1118.74] loss=1.83 avg=2.10\n",
            "[463 | 1121.07] loss=1.55 avg=2.10\n",
            "[464 | 1123.39] loss=2.12 avg=2.10\n",
            "[465 | 1125.72] loss=1.50 avg=2.09\n",
            "[466 | 1128.06] loss=1.68 avg=2.09\n",
            "[467 | 1130.39] loss=1.55 avg=2.08\n",
            "[468 | 1132.71] loss=2.05 avg=2.08\n",
            "[469 | 1135.04] loss=1.46 avg=2.08\n",
            "[470 | 1137.37] loss=1.75 avg=2.07\n",
            "[471 | 1139.70] loss=1.79 avg=2.07\n",
            "[472 | 1142.03] loss=2.00 avg=2.07\n",
            "[473 | 1144.36] loss=1.83 avg=2.07\n",
            "[474 | 1146.67] loss=1.39 avg=2.06\n",
            "[475 | 1149.00] loss=1.57 avg=2.06\n",
            "[476 | 1151.33] loss=1.84 avg=2.05\n",
            "[477 | 1153.67] loss=1.73 avg=2.05\n",
            "[478 | 1155.99] loss=1.87 avg=2.05\n",
            "[479 | 1158.31] loss=1.99 avg=2.05\n",
            "[480 | 1160.63] loss=1.66 avg=2.04\n",
            "[481 | 1162.96] loss=1.92 avg=2.04\n",
            "[482 | 1165.29] loss=1.83 avg=2.04\n",
            "[483 | 1167.61] loss=1.96 avg=2.04\n",
            "[484 | 1169.93] loss=2.40 avg=2.04\n",
            "[485 | 1172.25] loss=2.03 avg=2.04\n",
            "[486 | 1174.57] loss=1.93 avg=2.04\n",
            "[487 | 1176.90] loss=1.59 avg=2.04\n",
            "[488 | 1179.23] loss=2.03 avg=2.04\n",
            "[489 | 1181.55] loss=1.60 avg=2.03\n",
            "[490 | 1183.88] loss=1.73 avg=2.03\n",
            "[491 | 1186.21] loss=1.68 avg=2.03\n",
            "[492 | 1188.54] loss=1.93 avg=2.03\n",
            "[493 | 1190.87] loss=1.85 avg=2.02\n",
            "[494 | 1193.19] loss=1.83 avg=2.02\n",
            "[495 | 1195.51] loss=1.56 avg=2.02\n",
            "[496 | 1197.83] loss=1.89 avg=2.02\n",
            "[497 | 1200.16] loss=1.85 avg=2.01\n",
            "[498 | 1202.49] loss=1.79 avg=2.01\n",
            "[499 | 1204.81] loss=1.79 avg=2.01\n",
            "[500 | 1207.12] loss=1.85 avg=2.01\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d8f9f2-ab2b-4cea-f513-f0a01b7884e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth? \n",
            "The answer is no more than that, but I do not want to see another one. Small wonder. \n",
            "Page 462\n",
            "\n",
            "It would appear that the first two were coiling around the Tullys, sucking at their cords, \n",
            "as the largest were moving toward the Riverrun and the smallest toward the Whispering Woods. \n",
            "Let us not forget the two things that kept these two from coming together. The first was the \n",
            "coldness of their bondsmen. \n",
            "The second was their lord. \"No,\" he swore, \"I have nothing to do with the plans of the other gods. \n",
            "I am the sword of justice.\" \n",
            "For a moment Cersei was afraid to say more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "564ebb74-2ba5-403f-dc4b-c36ce1478d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}